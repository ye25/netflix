---
title: "Netflix Assignment - Marketing Analytics HW7"
author: "Grace(Ya) You, Ina(Mengyuan) Ye"
Due date: "March 23, 2018"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##PART 1: Data Management
+1)  Load the rating data into R and save as a data frame. Rename the columns "1",..,"5" as "Rocky1",.,"Rocky5"

```{r}


library(readr)
rockyDB <- read_csv("rockyDB.csv")

names(rockyDB)[2] <- "rocky1"
names(rockyDB)[3] <- "rocky2"
names(rockyDB)[4] <- "rocky3"
names(rockyDB)[5] <- "rocky4"
names(rockyDB)[6] <- "rocky5"

```


+b) The data contains ‘0’ values. This was used as a default rating for the case where consumers didn’t rate that particular Rocky movie. Why would representing missing values with ‘0’ cause problems for future analysis? (5 marks)

*The problem with 0 values is that when comparing the ratings for each movie. The algorithm of calculating mean, median, and even standard deviation would consider 0 as the lowest rating instead of NA value. For example, suppose rocky5 is a unpopular but highly-rated movie, putting 0 instead of null will misrepresent rocky5 to be low rated movie. 

+c) Replace 0 with NA
```{r}
rockyDB[rockyDB==0] <- NA
```


##PART 2: Data Exploration and Missing Data

+a) Compare the correlations between the ratings of each of the five movies using the cor command on your new data frame. Which movies are most similar? Which movie is most different from the others? Put the matrix of correlations and your response in the document. 

```{r}
library(corrplot)
library(gplots)
rockyDB_corr <- subset(rockyDB,select = -c(1))

cor <- cor(rockyDB_corr,use="complete.obs")

corrplot::corrplot.mixed(corr=cor,tl.pos="lt")

```
*We can see from the graph above that Rocky3 and Rock2 are most similar with each orthers, with 0.75 correlation. Rocky1 is most differenct than others because its average correlation with others is the lowest across all five movies. 

+b) Find the mean rating of each movie. Which is the best? Which is the worst? Answer in the document.
```{r}
means <- colMeans(rockyDB[,2:6],na.rm = TRUE)
print(round(means,digit=4))

```
*From the table above we can conclude that the rocky1 is the best with average rate of 3.77 and rock 5 has the lowest average rate at 3.03.


+c)Create a subset of your data frame that only contains consumers who rated Rocky 4. Find the mean rating of each movie in the data set. Compare with the results of the previous section. Why do you think the mean ratings changed? Answer in the document.
```{r}

rockyDB_valid4 <- subset(rockyDB, !is.na(rocky4))

print(colMeans(rockyDB[,2:6],na.rm = TRUE))
print(colMeans(rockyDB_valid4[,2:6],na.rm = TRUE))


```
*Within the subset group who rated rocky4, while the overall rank of the ratings in these 5 moives are the same, the average ratings for the other four movies are all higher. This could be that the people who watched rocky4 will tend to be more interested in the other four moives or the rocky4 movie audiences are less critical in moive rating.

+d) Why are some movies missing? What kind of bias might be caused by omitting incomplete data? Create a subset of your data by omitting all the NA observations (call this dataset “rockyDB_noNA”). Report the bias in the mean ratings from this subsetted data. Hint: Look at the na.omit function

```{r}


rockyDB_noNA <- na.omit(rockyDB)

colMeans(rockyDB_noNA[,2:6])

```
*Some movies missing is because some viewers might not watch all the 5 movies - they could be only interested in some type of the movies (as we shown in the correlation graph earlier, the types of these five movies are varies). By omitting the incomplete dataset, we will have less rating records for those who are interested in this specific type of movie (could be expert in the field, or could provide more insights and more critical in grading the movie - thus relatively lower scores),
*Compare to the mean value with only removing the NA values, we can see that the average ratings for all five movies have increased. This supports the bias mentioned above, that lower scores were omitted while those could potentially given by movie lovers that are experts in specific fields.


+e) Some missing data analysis instead build a statistical model based on the complete observations to ‘fill in’ the missing values. Why might the data still be biased after this procedure? (5 marks for discussion)

*The reason that there will still be biased after filling in the missing values with some model is that, the movies that have moissing ratings could be types that the viewers are not interested in or not familiar in. Thus, the ratings would be relatively hard to predict with the ratings of movies they favored, which also explained the biased in this strategy.

+f) Create a subset of your data by omitting NA observations in the “Rocky5” variable. For this subsetted dataset, create variables to denote whether ratings for “Rocky1”,.., “Rocky4” are missing (call these “isMissing1”, …, “isMissing4”). Check whether having missing ratings in “Rocky1”,…, “Rocky4” is correlated with ratings for “Rocky5”. Describe the missing data strategies if there is no statistically significant correlation, and if there is. Implement these strategies in your data, and create the final dataset (call this “rockyDB_impute”). (10 marks)

```{r}
rockyDB_noNA5 <- subset(rockyDB, !is.na(rocky5))
rockyDB_noNA5$isMissing1 <- ifelse(is.na(rockyDB_noNA5$rocky1),0,1)
rockyDB_noNA5$isMissing2 <- ifelse(is.na(rockyDB_noNA5$rocky2),0,1)
rockyDB_noNA5$isMissing3 <- ifelse(is.na(rockyDB_noNA5$rocky3),0,1)
rockyDB_noNA5$isMissing4 <- ifelse(is.na(rockyDB_noNA5$rocky4),0,1)

rockyDB_noNA5_cor <- subset(rockyDB_noNA5,select=-c(1:5))


cor_noNA5 <- cor(rockyDB_noNA5_cor,use="complete.obs")


corrplot::corrplot.mixed(corr=cor_noNA5,tl.pos="lt")
cor(rockyDB_noNA5_cor)

library(Hmisc)
rcorr(as.matrix(rockyDB_noNA5_cor))

cor.test(rockyDB_noNA5_cor$rocky5,rockyDB_noNA5_cor$isMissing1,method="pearson")
cor.test(rockyDB_noNA5_cor$rocky5,rockyDB_noNA5_cor$isMissing2,method="pearson")
cor.test(rockyDB_noNA5_cor$rocky5,rockyDB_noNA5_cor$isMissing3,method="pearson")
cor.test(rockyDB_noNA5_cor$rocky5,rockyDB_noNA5_cor$isMissing4,method="pearson")


#234 correlated 1 uncorrelated

rockyDB_impute <- subset(rockyDB,!is.na(rocky1))
rockyDB_impute <- subset(rockyDB_impute,!is.na(rocky5))
means_impute <- as.matrix(colMeans(rockyDB_impute,na.rm=TRUE))

rockyDB_impute$rockyy2 <- ifelse(is.na(rockyDB_impute$rocky2),mean(rockyDB$rocky2,na.rm=TRUE),rockyDB_impute$rocky2)
rockyDB_impute$rockyy3 <- ifelse(is.na(rockyDB_impute$rocky3),mean(rockyDB$rocky3,na.rm=TRUE),rockyDB_impute$rocky3)
rockyDB_impute$rockyy4 <- ifelse(is.na(rockyDB_impute$rocky4),mean(rockyDB$rocky4,na.rm=TRUE),rockyDB_impute$rocky4)

rockyDB_impute <- subset(rockyDB_impute,select=c(2,6:9))

names(rockyDB_impute)[3] <- "rocky2"
names(rockyDB_impute)[4] <- "rocky3"
names(rockyDB_impute)[5] <- "rocky4"
```


##Part 3: Predictive Modelling (40 marks)

Here we will try to determine whether Rocky 5 should be recommended to Netflix customers, based on their ratings of Rocky 1 through 4. The dependent variables is the rating of Rocky 5, and the independent variables are the ratings of the previous 4 Rocky movies. Treat the rating of Rocky 5 as a continuous variable (rather than a categorical variable). 

a) For the “rockyDB_noNA” data (with all complete observations), estimate and store a simple linear regression between the given dependent variable and the independent variables (Rocky5 ~ Rocky1 + Rocky2 + Rocky3 + Rocky4) (5 marks)
```{r}
lm_noNA<-lm(rocky5 ~ rocky1 + rocky2 + rocky3 + rocky4,data=rockyDB_noNA)
summary(lm_noNA)

```


b) For the “rockyDB_impute” data (with all complete observations), estimate and store a simple linear regression between the given dependent variable and the independent variables (Rocky5 ~ Rocky1 + Rocky2 + Rocky3 + Rocky4). Compare the regression coefficients between the two models. (5 marks)

```{r}
lm_impute<-lm(rocky5 ~ rocky1 + rocky2 + rocky3 + rocky4,data=rockyDB_impute)
summary(lm_impute)
```


c) To the regression model in part (b) above, add the variables for “isMissing2”, “isMissing3”, “isMissing4”. Does adding these variables improve the R-squared? What about the adjusted R-squared? (5 marks)
```{r}

rockyDB_noNA5 <-subset(rockyDB_noNA5,!is.na(rockyDB_noNA5$rocky1))
rockyDB_impute$isMissing2 <- rockyDB_noNA5$isMissing2
rockyDB_impute$isMissing3 <- rockyDB_noNA5$isMissing3
rockyDB_impute$isMissing4 <- rockyDB_noNA5$isMissing4

lm_add_missing <- lm(rocky5~.,data=rockyDB_impute)
summary(lm_add_missing)
```


d) Write code that estimates at least 20 statistical models with these predictor variables using the “rockyDB_impute” data. (15 marks) Hint: You can manually or automatically
generate a set of formulas, and then use those formulas in for different estimation procedures. For example, you can use transformations of the predictor variables (polynomials, log, interactions, treating them as categorical etc.). Below is some code that will automatically generate 4 formulas, you can expand it to estimate more models, or you can write the models down manually)

rocky1Vec = c('','+Rocky1','+poly(Rocky1,2)')
rocky2Vec = c('','+Rocky2','+poly(Rocky2,2)')
formulaSet = paste('Rocky5~1',
apply(expand.grid(rocky1Vec,rocky2Vec),1,paste,collapse=''))
lm(as.formula(formulaSet[1]),data=rockyDB_impute)

```{r}
a <- lm(rocky5~rocky1,data=rockyDB_impute)
b <- lm(rocky5~rocky1+rocky2+rocky3+rocky4,data=rockyDB_impute)
c <- lm(rocky5~rocky1+rocky2+rocky3,data=rockyDB_impute)
d <- lm(rocky5~rocky1+rocky2,data=rockyDB_impute)
e <- lm(rocky5~rocky1+isMissing2+isMissing3+isMissing4,data=rockyDB_impute)
f <- lm(rocky5~rocky1+rocky2*isMissing2+rocky3+rocky4,data=rockyDB_impute)
g <- lm(rocky5~rocky1+rocky2+rocky3*isMissing3+rocky4,data=rockyDB_impute)
h <- lm(rocky5~rocky1+rocky2+rocky3+rocky4*isMissing4,data=rockyDB_impute)
i <- lm(rocky5~rocky1+rocky2*isMissing2+rocky3*isMissing3+rocky4,data=rockyDB_impute)
j <- lm(rocky5~rocky1+rocky2&isMissing2+rocky3+rocky4*isMissing4,data=rockyDB_impute)
k <- lm(rocky5~rocky1+rocky2+rocky3+rocky4,data=rockyDB_impute)
l <- lm(rocky5~rocky1+rocky2*rocky3+rocky4,data=rockyDB_impute)
m <- lm(rocky5~rocky1+rocky2+rocky3*rocky4,data=rockyDB_impute)
n <- lm(log(rocky5)~rocky1+rocky2+rocky3+rocky4,data=rockyDB_impute)
o <- lm(log(rocky5)~log(rocky1)+rocky2+rocky3+rocky4,data=rockyDB_impute)
p <- lm(rocky5~rocky1^2+rocky2*isMissing2+rocky3+rocky4,data=rockyDB_impute)
q <- lm(rocky5~rocky1^2+rocky2*isMissing2+rocky3*isMissing3+rocky4,data=rockyDB_impute)
r <- lm(rocky5~rocky1+rocky2*rocky3+rocky4*isMissing4,data=rockyDB_impute)
s <- lm(rocky5~isMissing2+isMissing3+isMissing4,data=rockyDB_impute)
t <- lm(rocky5~rocky1+rocky2+rocky3*isMissing3,data=rockyDB_impute)







```


e) Split the dataset into a 80:20 training and validation set. Use these data sets to estimate and then evaluate the out-of-sample MSE of each model in part d. Report the table of out-of-sample MSEs for these models. (5 marks).
```{r}

library(rpart)
library(rpart.plot)
# partition
set.seed(1)  
train.index <- sample(c(1:dim(rockyDB_impute)[1]), dim(rockyDB_impute)[1]*0.8)  
train.df <- rockyDB_impute[train.index, ]
valid.df <- rockyDB_impute[-train.index, ]

t_a <- lm(rocky5~rocky1,data=train.df)
t_b <- lm(rocky5~rocky1+rocky2+rocky3+rocky4,data=train.df)
t_c <- lm(rocky5~rocky1+rocky2+rocky3,data=train.df)
t_d <- lm(rocky5~rocky1+rocky2,data=train.df)
t_e <- lm(rocky5~rocky1+isMissing2+isMissing3+isMissing4,data=train.df)
t_f <- lm(rocky5~rocky1+rocky2*isMissing2+rocky3+rocky4,data=train.df)
t_g <- lm(rocky5~rocky1+rocky2+rocky3*isMissing3+rocky4,data=train.df)
t_h <- lm(rocky5~rocky1+rocky2+rocky3+rocky4*isMissing4,data=train.df)
t_i <- lm(rocky5~rocky1+rocky2*isMissing2+rocky3*isMissing3+rocky4,data=train.df)
t_j <- lm(rocky5~rocky1+rocky2&isMissing2+rocky3+rocky4*isMissing4,data=train.df)
t_k <- lm(rocky5~rocky1+rocky2+rocky3+rocky4,data=train.df)
t_l <- lm(rocky5~rocky1+rocky2*rocky3+rocky4,data=train.df)
t_m <- lm(rocky5~rocky1+rocky2+rocky3*rocky4,data=train.df)
t_n <- lm(log(rocky5)~rocky1+rocky2+rocky3+rocky4,data=train.df)
t_o <- lm(log(rocky5)~log(rocky1)+rocky2+rocky3+rocky4,data=train.df)
t_p <- lm(rocky5~rocky1^2+rocky2*isMissing2+rocky3+rocky4,data=train.df)
t_q <- lm(rocky5~rocky1^2+rocky2*isMissing2+rocky3*isMissing3+rocky4,data=train.df)
t_r <- lm(rocky5~rocky1+rocky2*rocky3+rocky4*isMissing4,data=train.df)
t_s <- lm(rocky5~isMissing2+isMissing3+isMissing4,data=train.df)
t_t <- lm(rocky5~rocky1+rocky2+rocky3*isMissing3,data=train.df)


p_a<- predict(t_a,newdata = valid.df)
p_b<- predict(t_b,newdata = valid.df)
p_c<- predict(t_c,newdata = valid.df)
p_d<- predict(t_d,newdata = valid.df)
p_e<- predict(t_e,newdata = valid.df)
p_f<- predict(t_f,newdata = valid.df)
p_g<- predict(t_g,newdata = valid.df)
p_h<- predict(t_h,newdata = valid.df)
p_i<- predict(t_i,newdata = valid.df)
p_j<- predict(t_j,newdata = valid.df)
p_k<- predict(t_k,newdata = valid.df)
p_l<- predict(t_l,newdata = valid.df)
p_m<- predict(t_m,newdata = valid.df)
p_n<- predict(t_n,newdata = valid.df)
p_o<- predict(t_o,newdata = valid.df)
p_p<- predict(t_p,newdata = valid.df)
p_q<- predict(t_q,newdata = valid.df)
p_r<- predict(t_r,newdata = valid.df)
p_s<- predict(t_s,newdata = valid.df)
p_t<- predict(t_t,newdata = valid.df)


oos_a <- (mean(valid.df$rocky5-p_a))^2
oos_b <- (mean(valid.df$rocky5-p_b))^2
oos_c <- (mean(valid.df$rocky5-p_c))^2
oos_d <- (mean(valid.df$rocky5-p_d))^2
oos_e <- (mean(valid.df$rocky5-p_e))^2
oos_f <- (mean(valid.df$rocky5-p_f))^2
oos_g <- (mean(valid.df$rocky5-p_g))^2
oos_h <- (mean(valid.df$rocky5-p_h))^2
oos_i <- (mean(valid.df$rocky5-p_i))^2
oos_j <- (mean(valid.df$rocky5-p_j))^2
oos_k <- (mean(valid.df$rocky5-p_k))^2
oos_l <- (mean(valid.df$rocky5-p_l))^2
oos_m <- (mean(valid.df$rocky5-p_m))^2
oos_n <- (mean(valid.df$rocky5-p_n))^2
oos_o <- (mean(valid.df$rocky5-p_o))^2
oos_p <- (mean(valid.df$rocky5-p_p))^2
oos_q <- (mean(valid.df$rocky5-p_q))^2
oos_r <- (mean(valid.df$rocky5-p_r))^2
oos_s <- (mean(valid.df$rocky5-p_s))^2
oos_t <- (mean(valid.df$rocky5-p_t))^2

row_name<- c(letters[1:20])
col <- c(oos_a,oos_b,oos_c,oos_d,oos_e,oos_f,oos_g,oos_h,oos_i,oos_j,oos_k,oos_l,oos_m,oos_n,oos_o,oos_p,oos_q,oos_r,oos_s,oos_t)

table <- matrix(col,ncol=1,byrow=TRUE)
rownames(table) <- row_name
table <-as.data.frame(table)

print(table)


```

f) Find the model performs best in terms of out-of-sample MSE, and present the model in the document, and discuss the results. (5 marks)
```{r}
min(table$V1)
#this is from model m 

summary(t_m)
```
The result for model m, which the formula is rocky5 ~ rocky1 + rocky2 + rocky3 * rocky4, the results shows that most of input variables are at the absolute level of significances. The adjusted r-squared rate is 0.3831, which is at the similar level with the orignial linear model. As this model considers the interaction between rocky3 and rocky4, we can observe that people who watch rocky3 have less influence on overall rocky 5 ratings. But the audience who watch rocky4 and give good ratings on 4 may gave good ratings on rocky5.  